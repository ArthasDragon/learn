# 数据结构与算法之美

- [复杂度分析](#complexity)
- [数组](#array)
- [链表](#linkedList)

<h1 id="complexity">复杂度分析</h1>

## 大 O 复杂度表示法

T(n) = O(f(n))

- T(n) 代码执行时间
- n 数据规模的大小
- f(n) 每行代码执行的次数总和

> 大 O 时间复杂度实际上并不具体表示代码真正的执行时间， 而是表示代码执行时间随数据规模增长的变化趋势， 所以， 也叫作渐进时间复杂度， 简称时间复杂度。 

## 时间复杂度分析

三个比较实用的方法

1. 只关注循环执行次数最多的一段代码
2. 加法法则： 总复杂度等于量级最大的那段代码的复杂度
3. 乘法法则： 嵌套代码的复杂度等于嵌套内外的代码复杂度的乘积

![magnitude](./imgs/magnitude.png)

分为多项式量级和非多项式量级（O(2<sup>n</sup>)、 O(n<sup>2</sup>)）

### O(logn)

> 因为对数之间可以相互转换， 所以统一用 logn 表示

```javascript
i = 1;
while (i <= n) {
    i = i * 2;
}
```

### O(m+n)， O(m\*n)

## 空间复杂度分析

> 又叫渐进空间复杂度，表示算法的存储空间与数据规模之间的增长关系。

常见空间复杂度：O(1)，O(n)，O(n<sup>2</sup>)

## 最好、最坏、平均、均摊时间复杂度

```javascript
  // n 表示数组 array 的长度
  int find(int[] array, int n, int x) {
    int i = 0;
    int pos = -1;
    for (; i < n; ++i) {
      if (array[i] == x) {
        pos = i;
        break;
      }
    }
    return pos;
  }
```

> 最好最坏复杂度意义不大：最好情况时间复杂度：O(1)、最坏情况时间复杂度：O(n)

平均情况时间复杂度（加权平均时间复杂度）

![averageComplexity](./imgs/averageComplexity.png)


均摊时间复杂度 （摊还分析法）

![desc1](./imgs/desc1.png)

<h1 id="array">数组</h1>

> 数组是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。

线性表（只有简单的前后关系）

![linearList](./imgs/linearList.png)

非线性表（不只有简单的前后关系喽）

![noLinearList](./imgs/noLinearList.png)

当计算机需要访问数组中的某个元素时，它会首先通过寻址公式，计算出该元素存储的内存地址。

```javascript
  a[i]_address = base_address + i * data_type_size
```

根据下标随机访问的时间复杂度是O(1)

## 低效的插入和删除

插入操作：

> 如果要向有序数组第 k 个位置插入一个数据，需要将之后的数据依次后移一位，平均情况时间复杂度为O(n)
> 
> 如果数组无序，可以将原来第 k 个位置的元素放到末尾，新元素放到第 k 个位置即可。

删除操作：

> 如果要删除数组第 k 个位置的数据，需要将之后的数据依次前移一位，平均情况时间复杂度为O(n)

## 警惕数组的访问越界问题

> c语言中下标越界仍然可以访问到该地址所对应的数据！

## 容器能否完全替代数组？

个人认为，ArrayList最大的优势是可以将很多数组操作的细节封装起来。还有一个优势是动态扩容。

直接使用数组的几种情况

![useArray](./imgs/useArray.png)

总结一下，对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。

## 为什么下标从0开始

从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。前面也讲到，如果用 a 来表示数组的首地址，a[0] 就是偏移为 0 的位置，也就是首地址，a[k] 就表示偏移 k 个 type_size 的位置，所以计算 a[k] 的内存地址只需要用这个公式：

```
  a[k]_address = base_address + k * type_size
```

但是，如果数组从 1 开始计数，那我们计算数组元素 a[k] 的内存地址就会变为：

```
  a[k]_address = base_address + (k-1)*type_size
```

每次随机访问数组元素都多了一次减法运算，对于CPU来说就是多了一次减法指令。

> 不过我认为，上面解释得再多其实都算不上压倒性的证明，说数组起始编号非 0 开始不可。所以我觉得最主要的原因可能是历史原因。

> 实际上，很多语言中数组也并不是从 0 开始计数的，比如 Matlab。甚至还有一些语言支持负数下标，比如 Python。

<h1 id="linkedList">链表</h1>

LRU 缓存淘汰算法

> 缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 FIFO（First In，First Out）、最少使用策略 LFU（Least Frequently Used）、最近最少使用策略 LRU（Least Recently Used）。

## 五花八门的链表结构

数组与链表的区别

> 底层的存储结构

从图中可以看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。

而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。

![array&linkedList](./imgs/array&linkedList.png)

三种最常见的链表结构：单链表、双向链表、循环链表


### 单链表：

把内存块称为<strong>“结点”</strong>。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。如图所示，我们把这个记录下个结点地址的指针叫作 <strong>后继指针 next</strong>

![singleLinkedList](./imgs/singleLinkedList.png)

第一个结点（头结点），最后一个结点（尾结点）比较特殊

其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。

在链表中插入和删除一个数据是非常快速的，我们只需要考虑相邻结点的指针改变，所以对应的时间复杂度是 O(1)。

![insAndDelLinkedList](./imgs/insAndDelLinkedList.png)

随机访问第 k 个元素没有数组高效，因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点，时间复杂度为 O(n)。

### 循环链表：

> 是一种特殊的单链表

![circularLinkedList](./imgs/circularLinkedList.png)

和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的[约瑟夫问题](https://zh.wikipedia.org/wiki/%E7%BA%A6%E7%91%9F%E5%A4%AB%E6%96%AF%E9%97%AE%E9%A2%98)。尽管用单链表也可以实现，但是用循环链表实现的话，代码就会简洁很多。

### 双向链表



